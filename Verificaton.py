import os
import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.models import load_model
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import skew, kurtosis

# ======================================
# éªŒè¯è„šæœ¬é…ç½®
# ======================================
# âš ï¸ 1. è¯·ä¿®æ”¹ä¸ºæ‚¨çš„æ–°æ•°æ®æ ¹ç›®å½•
NEW_DATA_ROOT = r"G:\pythonfiles\Driving Behavior\pythonProject\Verification_Data"
# âš ï¸ 2. è¯·ç¡®ä¿ SAVING_DIR ä¸è®­ç»ƒè„šæœ¬ä¸­çš„ä¸€è‡´
SAVING_DIR = "saving"
# 3. åºåˆ—é•¿åº¦å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´
TIME_STEPS = 10

# å®šä¹‰ç”¨äºä¿å­˜ç»“æœçš„ç›®å½•
# CSV æ–‡ä»¶å°†ä¿å­˜åˆ° NEW_DATA_ROOT/Verification_Results ç›®å½•ä¸‹
VERIFICATION_RESULTS_DIR = os.path.join(NEW_DATA_ROOT, "Verification_Results")

# ======================================
# éƒ¨ä»¶ç»“æ„ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
# ======================================
STRUCTURES = {
    "Seat belt": {"Belly": ["Crush", "Normal"], "Chest": ["Crush", "Normal"]},
    "Steering wheel": {"3": ["Grip heavily", "Leave with both hands", "Normal"],
                       "9": ["Grip heavily", "Leave with both hands", "Normal"]},
    "Pedal": {"": ["Step", "Normal"]},
    "Seat cushion": {"1.1": ["Back", "Forward", "Left", "Right", "Normal"],
                     "1.2": ["Back", "Forward", "Left", "Right", "Normal"],
                     "2.1": ["Back", "Forward", "Left", "Right", "Normal"],
                     "2.2": ["Back", "Forward", "Left", "Right", "Normal"]}
}


# ======================================
# è¾…åŠ©å‡½æ•°ï¼ˆæ¥è‡ªåŸè®­ç»ƒè„šæœ¬ï¼‰
# ======================================

def extract_features_from_csv(file_path):
    try:
        df = pd.read_csv(file_path, skiprows=8, header=None)
        df.columns = ['time', 'cap']
        if len(df) < 10:
            return None
    except Exception:
        return None

    x = df['cap'].values
    feats = {
        'mean': np.mean(x),
        'std': np.std(x),
        'min': np.min(x),
        'max': np.max(x),
        'range': np.ptp(x),
        'skew': skew(x),
        'kurtosis': kurtosis(x),
        'energy': np.sum(x ** 2) / len(x),
        'median': np.median(x),
        'iqr': np.percentile(x, 75) - np.percentile(x, 25)
    }

    if len(df) > 1:
        dt = np.mean(np.diff(df['time'].values))
        deriv = np.gradient(x)
        feats['sampling_interval'] = dt
        feats['derivative_mean'] = np.mean(deriv)
        feats['derivative_std'] = np.std(deriv)
    else:
        feats['sampling_interval'] = 0
        feats['derivative_mean'] = 0
        feats['derivative_std'] = 0

    return feats


def load_component_data(root_folder, substructure):
    data, labels = [], []
    for channel, states in substructure.items():
        channel_path = os.path.join(root_folder, channel)
        if not os.path.isdir(channel_path) and channel != "":
            continue
        for state in states:
            if channel == "":
                state_path = os.path.join(root_folder, state)
            else:
                state_path = os.path.join(channel_path, state)
            if not os.path.isdir(state_path):
                continue
            for file in os.listdir(state_path):
                if not file.endswith(".csv"):
                    continue
                feats = extract_features_from_csv(os.path.join(state_path, file))
                if feats is not None:
                    feats["channel"] = channel
                    data.append(feats)
                    labels.append(state)
    df = pd.DataFrame(data)
    df["label"] = labels
    return df


def create_sequences(X, y, time_steps):
    X_seq, y_seq = [], []
    X_array = X.values
    y_array = y

    for i in range(len(X) - time_steps + 1):
        X_seq.append(X_array[i:(i + time_steps)])
        y_seq.append(y_array[i + time_steps - 1])

    return np.array(X_seq), np.array(y_seq)


class LSTMWrapper:
    def __init__(self, model, scaler, le, time_steps):
        self.model = model
        self.scaler = scaler
        self.le = le
        self.time_steps = time_steps

    def predict(self, X_df):
        X_scaled = self.scaler.transform(X_df)
        X_scaled_df = pd.DataFrame(X_scaled, columns=X_df.columns)

        X_seq, _ = create_sequences(
            X_scaled_df,
            pd.Series([0] * len(X_scaled_df), index=X_scaled_df.index),
            self.time_steps
        )

        if len(X_seq) == 0:
            return np.array(["Normal"] * len(X_df))

        y_pred_probs = self.model.predict(X_seq, verbose=0)
        y_pred_encoded = np.argmax(y_pred_probs, axis=1)

        initial_preds_count = len(X_df) - len(y_pred_encoded)
        initial_preds = ["Normal"] * initial_preds_count

        final_preds = self.le.inverse_transform(y_pred_encoded)

        return np.array(initial_preds + final_preds.tolist())


# ======================================
# ä¸»é¢„æµ‹é€»è¾‘ (å·²æ›´æ–°ï¼ŒåŒ…å«æ··æ·†çŸ©é˜µCSVè¾“å‡º)
# ======================================
def validate_new_data(root_folder, structures, saving_dir, time_steps, results_dir):
    component_preds = {}
    print(f"ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹å’Œæ–°æ•°æ® (ROOT: {root_folder})")

    # ç¡®ä¿ç»“æœä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(results_dir, exist_ok=True)
    print(f"ğŸ’¾ ç»“æœæ–‡ä»¶å°†ä¿å­˜åˆ°ç›®å½•: {results_dir}")

    # 1. é€ä¸ªéƒ¨ä»¶åŠ è½½æ¨¡å‹å’Œæ•°æ®ï¼Œå¹¶è¿›è¡Œé¢„æµ‹
    for component, structure in structures.items():
        name = component.replace(" ", "_")
        folder = os.path.join(root_folder, component)

        # 1.1 åŠ è½½æ–°æ•°æ®
        df_new = load_component_data(folder, structure)

        # æå–ç‰¹å¾æ•°æ®ï¼Œç”¨äºé¢„æµ‹
        X_new = df_new.drop(columns=["label", "channel"])
        y_true = df_new["label"].values  # çœŸå®æ ‡ç­¾ç”¨äºè¯„ä¼°

        print(f"\nğŸ“‚ æ­£åœ¨å¤„ç† {component}ï¼Œæ ·æœ¬æ•°: {df_new.shape[0]}")

        if df_new.empty:
            print(f"âš ï¸ {component} æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
            continue

        # 1.2 åŠ è½½æ¨¡å‹æ–‡ä»¶
        model_type = "RF"
        y_pred = None

        if component == "Seat cushion":
            # LSTM æ¨¡å‹
            model_path = os.path.join(saving_dir, f"{name}_LSTM_model.h5")
            scaler_path = os.path.join(saving_dir, f"{name}_LSTM_scaler.pkl")
            le_path = os.path.join(saving_dir, f"{name}_LSTM_le.pkl")

            if not os.path.exists(model_path):
                print(f"âŒ æ‰¾ä¸åˆ° LSTM æ¨¡å‹æ–‡ä»¶: {model_path}ï¼Œè·³è¿‡ã€‚")
                continue

            model = load_model(model_path)
            scaler = joblib.load(scaler_path)
            le = joblib.load(le_path)

            # ä½¿ç”¨ LSTMWrapper è¿›è¡Œé¢„æµ‹
            lstm_wrapper = LSTMWrapper(model, scaler, le, time_steps)
            y_pred = lstm_wrapper.predict(X_new)
            model_type = "LSTM"

        else:
            # RF æ¨¡å‹
            model_path = os.path.join(saving_dir, f"{name}_RF_model.pkl")
            if not os.path.exists(model_path):
                print(f"âŒ æ‰¾ä¸åˆ° RF æ¨¡å‹æ–‡ä»¶: {model_path}ï¼Œè·³è¿‡ã€‚")
                continue

            model = joblib.load(model_path)
            y_pred = model.predict(X_new)

        # 1.3 å­˜å‚¨é¢„æµ‹ç»“æœ
        df_new["pred"] = y_pred
        component_preds[component] = df_new

        # --- ä¿å­˜Seat Cushionçš„é¢„æµ‹ç»“æœåˆ°CSV (ç”¨äºè¯Šæ–­) ---
        if component == "Seat cushion":
            output_path = os.path.join(results_dir, f"{name}_raw_predictions.csv")
            df_new[['label', 'pred']].to_csv(output_path, index=False)
            print(f"ğŸ“„ å·²å°† {component} çš„åŸå§‹é¢„æµ‹ç»“æœä¿å­˜åˆ°: {output_path}")
        # -------------------------------------------------------------------

        print(f"âœ… {component} é¢„æµ‹å®Œæˆ ({model_type})")

        # 1.4 æŠ¥å‘Šè¯¥éƒ¨ä»¶çš„æ€§èƒ½
        print(f"\n--- {component} ç‹¬ç«‹æŠ¥å‘Š (æ–°æ•°æ®) ---")
        print(classification_report(y_true, y_pred, zero_division=0))

        # 1.5 è®¡ç®—å¹¶ä¿å­˜æ··æ·†çŸ©é˜µ

        # æå–æ‰€æœ‰å¯èƒ½æ ‡ç­¾å¹¶æ’åº
        all_possible_labels_set = set()
        for states in structure.values():
            all_possible_labels_set.update(states)
        all_labels = sorted(list(all_possible_labels_set))

        # è®¡ç®—æ··æ·†çŸ©é˜µ (ä½¿ç”¨æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾ä½œä¸ºç»´åº¦)
        cm = confusion_matrix(y_true, y_pred, labels=all_labels)

        # è½¬æ¢ä¸ºç™¾åˆ†æ¯”å½¢å¼ (æŒ‰è¡Œå½’ä¸€åŒ–)
        row_sums = cm.sum(axis=1)[:, np.newaxis]
        cm_normalized = np.divide(cm.astype('float'),
                                  row_sums,
                                  out=np.zeros_like(cm.astype('float')),
                                  where=row_sums != 0)

        cm_percent_df = pd.DataFrame(cm_normalized * 100,
                                     index=[f"True_{lab}" for lab in all_labels], # æ˜ç¡®è¡Œæ˜¯çœŸå®æ ‡ç­¾
                                     columns=[f"Pred_{lab}" for lab in all_labels]) # æ˜ç¡®åˆ—æ˜¯é¢„æµ‹æ ‡ç­¾

        # ä¿å­˜æ··æ·†çŸ©é˜µæ•°æ®åˆ° CSV
        cm_output_path = os.path.join(results_dir, f"{name}_normalized_cm.csv")
        cm_percent_df.to_csv(cm_output_path)
        print(f"ğŸ’¾ å·²å°† {component} å½’ä¸€åŒ–æ··æ·†çŸ©é˜µæ•°æ®ä¿å­˜åˆ°: {cm_output_path}")


        print(f"\nğŸ“¢ {component} ç‹¬ç«‹æ··æ·†çŸ©é˜µ (è¡Œå½’ä¸€åŒ–ç™¾åˆ†æ¯”):")
        # è¾“å‡ºåˆ°æ§åˆ¶å°
        print(cm_percent_df.to_string())

        # ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­å›¾
        plt.figure(figsize=(7, 6))
        cmap_style = 'Blues' if model_type == 'RF' else 'Purples'
        sns.heatmap(cm_percent_df.values, annot=True, fmt='.2f', cmap=cmap_style,
                    xticklabels=all_labels, yticklabels=all_labels)
        plt.title(f"[New Data] {component} Normalized Confusion Matrix (%)")
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.show()

    # 2. èåˆé¢„æµ‹ç»“æœ (ä¿æŒä¸å˜)
    if not component_preds:
        print("\næ— æ³•è¿›è¡Œèåˆï¼šæ²¡æœ‰éƒ¨ä»¶æ•°æ®æˆ–æ¨¡å‹åŠ è½½å¤±è´¥ã€‚")
        return

    print("\nğŸ”— å¼€å§‹èåˆé¢„æµ‹ç»“æœ...")

    min_len = min([len(df) for df in component_preds.values()])
    true_combined, pred_combined = [], []

    for i in range(min_len):
        t_list, p_list = [], []

        # éå†æ‰€æœ‰éƒ¨ä»¶ï¼Œæ”¶é›†çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
        for component in structures.keys():
            if component in component_preds:
                df = component_preds[component]
                idx = min(i, len(df) - 1)

                true_label = df.iloc[idx]["label"]
                pred_label = df.iloc[idx]["pred"]

                if true_label != "Normal": t_list.append(true_label)
                if pred_label != "Normal": p_list.append(pred_label)

        true_state = "+".join(sorted(t_list)) if t_list else "Normal"
        pred_state = "+".join(sorted(p_list)) if p_list else "Normal"

        true_combined.append(true_state)
        pred_combined.append(pred_state)

    # 3. è¾“å‡ºèåˆæŠ¥å‘Š (åŒ…å«æœ€ç»ˆæ··æ·†çŸ©é˜µCSVè¾“å‡º)
    all_combined_labels = sorted(list(set(true_combined + pred_combined)))

    print("\n\n#####################################################")
    print("      âœ¨ æ–°æ•°æ®ï¼šå¤šéƒ¨ä»¶èåˆåˆ†ç±»æŠ¥å‘Š (LSTM + RF) âœ¨         ")
    print("#####################################################")
    print(classification_report(true_combined, pred_combined, zero_division=0))

    cm_final = confusion_matrix(true_combined, pred_combined, labels=all_combined_labels)

    row_sums = cm_final.sum(axis=1)[:, np.newaxis]
    cm_final_normalized = np.divide(cm_final.astype('float'),
                                    row_sums,
                                    out=np.zeros_like(cm_final.astype('float')),
                                    where=row_sums != 0)

    cm_final_percent_df = pd.DataFrame(cm_final_normalized * 100,
                                       index=[f"True_{lab}" for lab in all_combined_labels],
                                       columns=[f"Pred_{lab}" for lab in all_combined_labels])

    # ä¿å­˜æœ€ç»ˆèåˆçš„æ··æ·†çŸ©é˜µæ•°æ®åˆ° CSV
    final_cm_output_path = os.path.join(results_dir, "Combined_final_normalized_cm.csv")
    cm_final_percent_df.to_csv(final_cm_output_path)
    print(f"ğŸ’¾ å·²å°† **æœ€ç»ˆèåˆ** å½’ä¸€åŒ–æ··æ·†çŸ©é˜µæ•°æ®ä¿å­˜åˆ°: {final_cm_output_path}")


    plt.figure(figsize=(10, 8))
    sns.heatmap(cm_final_percent_df.values, annot=True, fmt='.2f', cmap='magma',
                xticklabels=all_combined_labels, yticklabels=all_combined_labels)
    plt.title("New Data: Final Combined Normalized Confusion Matrix (%)")
    plt.xlabel("Predicted Combined State")
    plt.ylabel("True Combined State")
    plt.show()


if __name__ == "__main__":
    validate_new_data(NEW_DATA_ROOT, STRUCTURES, SAVING_DIR, TIME_STEPS, VERIFICATION_RESULTS_DIR) 
